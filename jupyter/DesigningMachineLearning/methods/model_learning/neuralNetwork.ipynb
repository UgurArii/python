{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f42f66-945a-4d61-ac34-9cf8bf945a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0161118-fcef-474c-95e8-44a716f74bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetMLP(object):\n",
    "    def __init__(self, n_output, n_features, n_hidden=30, l1=0.0, l2=0.0,\n",
    "                 epochs=500, eta=0.001, alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        \n",
    "    def _encode_labels(self, y, k):\n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "    def _initialize_weight(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = reshape(self.n_hidden, self.n_features +1 )\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.hidden +1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        #return 1.0(1.0 + np.exp(-z))\n",
    "        return expit(x)\n",
    "    \n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        if how=='column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1]+1))\n",
    "            X_new[:,1:] = X\n",
    "        \n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0]+1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "           \n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "    \n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        a1 = self._add_bias_nit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z2)\n",
    "        return a1, z2, z3, a3\n",
    "            \n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\" Compute L2-regularization cost\"\"\"\n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) + np.sum(w2[:, 1:]** 2))\n",
    "    \n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1 - y_enc) * np.log(1-output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "    \n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \n",
    "        #backpropagation\n",
    "        sigma3 = a3 - y_emc\n",
    "        s2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(igma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "        \n",
    "        #regularize\n",
    "        grad1[:, 1:] += (w1[:, 1:] * (self.l1 + self.l2))\n",
    "        grad2[:, 1:] += (w2[:, 1:] * (self.l1 + self.l2))\n",
    "        \n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n''Use X[:,None] for 1-feature classification,''\\nor X[[i]] for 1-sample classification')\n",
    "        a1, z2, a2, z2, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "        \n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            #adaptive learning rate\n",
    "            self.eta /= (1+ self.decrease_const * i)\n",
    "            \n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "                \n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_data = X_data[idx], y_data[idx]\n",
    "                \n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            \n",
    "            for idx in mini:\n",
    "                #feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X[idx], self.w1, self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output = a3,\n",
    "                                      w1 = self.w1,\n",
    "                                      w2 = self.w2)\n",
    "                self.cost_.append(cost)\n",
    "                \n",
    "                #compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2, a3=a3, z2=z2, y_enc = y_enc[:, idx],\n",
    "                                                 w1 = self.w1,\n",
    "                                                 w2 = self.w2\n",
    "                 \n",
    "                                                 )\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grand2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "                \n",
    "            return self\n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a69f3a9-12b3-4321-b189-65fcf77c5d9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetMLP' object has no attribute '_initialize_weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3989/3346392374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3989/2756542792.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_output, n_features, n_hidden, l1, l2, epochs, eta, alpha, decrease_const, shuffle, minibatches, random_state)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetMLP' object has no attribute '_initialize_weights'"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "nn = NeuralNetMLP(3, X.shape[1],n_hidden=50, epochs=100, alpha=.001)\n",
    "nn.fit(X, y)\n",
    "plt.plot(range(len(nn.cost_)), nn.cost_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abfcb4-d4af-4b59-ae17-80eeb1ad4935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
